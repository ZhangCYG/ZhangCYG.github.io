<!DOCTYPE html>
<html lang="en">
<head>
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-0.872831425-0.8"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'UA-0.872831425-0.8');
	</script>

	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

	<meta content="en-us" http-equiv="Content-Language">
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
	<title>Chenyangguang Zhang</title>
	<meta charset="utf-8">
	<style type="text/css">
		.container {
			zoom: 1;
			margin-left: auto;
			margin-right: auto;
			vertical-align: middle;
			text-align: left;
			width: 100%;
			max-width: 800px;
		}
		body {font-family: Arial}
		a {text-decoration:none}
		a:any-link{color: darkred}
     	a:link{color:darkred;}
		a:visited{color:darkred;}
        a:hover{color:darkorange;}

	</style>
</head>


<body style="width:66%;margin:auto" bgcolor="#FFFFFF">
	<table border="0" id="table1" style="margin-left: 8px">
		<tbody>
			<tr>
				<td width="323">
					<p align="center"><font face="Arial"><img border="0" src="figs/me.jpg" height="224"></font></p>
				</td>
				<td >
					<p ><font face="Arial" style="font-size: 26pt;" >&nbsp;Chenyangguang (Cyrus) Zhang (张晨阳光)<span lang="zh-cn"></span></font></p>

					<p style="margin-top: 3mm;margin-bottom: 0.5mm"><font face="Arial" style="font-size: 12pt;" >&nbsp; Master student at Department of Automation, Tsinghua University</font></p>
					<p style="margin-top: 3mm;margin-bottom: 0.5mm"><font face="Arial" style="font-size: 12pt;" >&nbsp; Incoming PhD student at <a href="https://cvg.ethz.ch/index">CVG</a>, ETH Zurich</font></p>
					<p style="margin-top: 3mm;margin-bottom: 0.5mm"><font face="Arial" style="font-size: 12pt;">&nbsp; <b>Email</b>: zcyg22@mails.tsinghua.edu.cn</font></p>
					&nbsp;
					<a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=-sCslRcAAAAJ" style="font-size: 12pt;"><b>Google Scholar</b></a> &bull;
					<a href="https://github.com/ZhangCYG" style="font-size: 12pt;"><b>GitHub</b></a>&bull;
					<a href="https://www.linkedin.com/in/chenyangguangzhang/" style="font-size: 12pt;"><b>LinkedIn</b></a>
				</td>

			</tr>
		</tbody>
	</table>

<table style="margin-left: 55px;margin-right: 55px">
		<tbody>
			<td>
				<td style="border-style: none; border-width: medium;">
					<p style="margin-top: 3px; margin-bottom: 3px;"><font face="Arial" style="font-size: 12pt;">
						I am a final year Master student at BBNC Lab, Department of Automation, Tsinghua University, supervised by Prof. Dr. <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm">Xiangyang Ji</a>.
						I got my bachelor's degree also from Department of Automation, Tsinghua University.
						I'm a incoming PhD student at <a href="https://cvg.ethz.ch/index">CVG</a>, ETH Zurich, supervised by Prof. Dr. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>.
						<br>
						<br>
						My research interest lies in <b style="color: darkorange">3D computer vision</b>,
						especially <b style="color: darkorange">scene reconstruction and understanding, human-scene interaction and pose estimation</b>.
						<br>
						<br>
						Some of my close colllaborators include PD Dr. <a href="https://federicotombari.github.io/">Federico Tombari</a>, Dr. <a href="https://francisengelmann.github.io/">Francis Engelmann</a>, Dr. <a href="https://shangbuhuan13.github.io/">Yan Di</a>, Dr. <a href="https://scholar.google.com.sg/citations?user=htu3c7wAAAAJ&hl=zh-CN">Gu Wang</a>, <a href="https://fangjinhuawang.github.io/">Fangjinhua Wang</a> and <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=J4u6VicAAAAJ">Ruida Zhang</a>.
						<br>
						<br>
						I am invited as a reviewer for CVPR, ICCV and ECCV. I'm a student member of Chinese Society of Image and Graphics (CSIG).
						<br>
						<br>
					</font></p>
				</td>
			</td>
		</tbody>
</table>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
		News</font></b></p>
		<font style="font-size: 12pt; line-height: 1.25;">
			<ul style="margin-left: 60px;margin-top: -10px">
				<li>[Mar, 2025] Glad to start an internship at <a href="https://dexmal.com/">Dexmal</a> doing something interesting about whole body control.</li>
				<li>[Mar, 2025] My co-authored work <a href="https://ieeexplore.ieee.org/abstract/document/10937121">GDRNPP</a> is accepted by <b>TPAMI</b>. It has been SOTA on BOP challenge since 2022.</li>
				<li>[Feb, 2025] My first-authored paper <a href="https://arxiv.org/pdf/2503.19199">OpenFunGraph</a> is accepted by <b>CVPR 2025</b>. It proposes a functional scene graph representation to model indoor functionality and interaction.</li>
				<li>[Feb, 2025] Two co-authored papers are accepted by <b>CVPR 2025</b>. <a href="https://arxiv.org/pdf/2411.16106">UNOPose</a> for unseen object pose estimation with an RGB-D reference. <a href="https://arxiv.org/pdf/2503.15110">GIVEPose</a> for RGB-based category-level object pose estimation.</li>
				<li>[Oct, 2024] Two co-authored papers are presented at <b>ECCV 2024</b>. <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04261.pdf">D-SCo</a> for diffusion-based hand-held object reconstruction.  <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/03696.pdf">LaPose</a> for RGB-based category-level object pose estimation. </li>
				<li>[May, 2024] My co-first-authored paper <a href="https://ieeexplore.ieee.org/abstract/document/10611208">RaSim</a> is presented in <b>ICRA 2024</b>. It involves a useful pipeline for simulating real-world depth scan and completing defected depth maps.</li>
				<li>[Feb, 2024] Three my (co-)first-authored papers are accepted by <b>CVPR 2024</b>, including <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MOHO_Learning_Single-view_Hand-held_Object_Reconstruction_with_Multi-view_Occlusion-Aware_Supervision_CVPR_2024_paper.pdf">MOHO</a> for single-view hand-held object reconstruction without 3D supervision; <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_KP-RED_Exploiting_Semantic_Keypoints_for_Joint_3D_Shape_Retrieval_and_CVPR_2024_paper.pdf">KP-RED</a> for shape analysis without part annotations; <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Di_ShapeMatcher_Self-Supervised_Joint_Shape_Canonicalization_Segmentation_Retrieval_and_Deformation_CVPR_2024_paper.pdf">ShapeMatcher</a> for indoor shape analysis from arbitrary poses.</li>
				<li>[Feb, 2024] I'm approved to enroll as an project mobility student to <b>CVG, ETH Zurich</b>. Great thanks for Prof. Marc Pollefeys for giving this opportunity!</li>
				<li>[Oct, 2023] Our work GPose2023 <b>wins BOP Challenge 2023</b>, ICCV R6D Workshop. It's honored for me to give an oral presentation in <b>ICCV 2023</b>.</li>
				<li>[Sept, 2023] My first-authored paper <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/b2876deb92cbd098219a10da25671577-Paper-Conference.pdf">DDF-HO</a> on single-view hand-held object reconstruction is accepted by <b>NeurIPS 2023</b>.</li>
				<li>[July, 2023] My co-first-authored paper <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Di_U-RED_Unsupervised_3D_Shape_Retrieval_and_Deformation_for_Partial_Point_ICCV_2023_paper.pdf">U-RED</a> on unsupervised shape retrieval and deformation in indoor scenes is accepted by <b>ICCV 2023</b>.</li>
				<!-- <li>[July, 2023] I have an oral presentation about paper SST in <b>ICME 2023</b>.</li> -->
				<!-- <li>[March, 2023] My first-authored paper <a href="https://ieeexplore.ieee.org/abstract/document/10219928">SST</a> on end-to-end real-time indoor scene reconstruction is accepted by <b>ICME 2023 (oral)</b>.</li> -->
				<!-- <li>[October, 2022] Our GDRNPP on object pose estimation wins the majority of <b>best awards in BOP Challenge 2022, ECCV R6D Workshop</b>.</li> -->
			</ul>
		</font>

<table style="width:80%;margin-left: 55px;border-spacing: 0;">
	<tbody>
	<tr>
		<p style="margin-left: 60px;margin-bottom: 10px"><b><font face="Arial" size="5">
		Selected Publications and Contests</font></b></p>
	</tr>

	<tr>
		<td colspan="2">
			<b><font face="Arial" size="4" style="color: rgb(107, 13, 60)">
				Theme 1: Reconstructing and Understanding Human-Scene Interaction </font></b><br>
			</td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/openfungraph.jpg" height="140" width="240" alt="MOHO"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces</strong><br>
					<span style="text-decoration: underline">Chenyangguang Zhang</span>, Alexandros Delitzas, Fangjinhua Wang, Ruida Zhang, Xiangyang Ji, Marc Pollefeys, Francis Engelmann<br>
					<em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2025. <br> 		
					<b><a href="https://arxiv.org/pdf/2503.19199">Paper</a></b>
					<!-- <b><a href="https://github.com/ZhangCYG/MOHO">Code</a></b> | -->
					<!-- <a href="figs\moho_poster.pdf"><b>Poster</b></a> -->
			</p></td></tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/MOHO.PNG" height="140" width="240" alt="MOHO"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision</strong><br>
					<span style="text-decoration: underline">Chenyangguang Zhang*</span>, Guanlong Jiao*, Yan Di, Gu Wang, Ziqin Huang, Ruida Zhang, Fabian Manhardt, Bowen Fu, Federico Tombari, Xiangyang Ji<br>
					<em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br> 		
					<b><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_MOHO_Learning_Single-view_Hand-held_Object_Reconstruction_with_Multi-view_Occlusion-Aware_Supervision_CVPR_2024_paper.pdf">Paper</a></b> |
					<b><a href="https://github.com/ZhangCYG/MOHO">Code</a></b> |
					<a href="figs\moho_poster.pdf"><b>Poster</b></a>
			</p></td></tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/ddfho.PNG" height="140" width="240" alt="ddfho"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field</strong><br>
					<span style="text-decoration: underline">Chenyangguang Zhang*</span>, Yan Di*, Ruida Zhang*, Guangyao Zhai, Fabian Manhardt, Federico Tombari, Xiangyang Ji<br>
					<em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023. <br> 		
					<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/b2876deb92cbd098219a10da25671577-Paper-Conference.pdf"><b>Paper</b></a> |
					<b><a href="https://github.com/ZhangCYG/DDFHO">Code</a></b> |
					<a href="figs\ddfho_poster.pdf"><b>Poster</b></a>
			</p></td></tr>

	<tr>
		<td colspan="2">
			<b><font face="Arial" size="4" style="color: rgb(107, 13, 60)">
				Theme 2: Object-Centric Indoor Scene Reconstruction</font></b><br>
			</td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/shapemaker.PNG" height="140" width="240" alt="shapemaker"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >ShapeMatcher: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation</strong><br>
					Yan Di*, <span style="text-decoration: underline">Chenyangguang Zhang</span>*, Chaowei Wang*, Ruida Zhang, Guangyao Zhai, Yanyan Li, Bowen Fu, Xiangyang Ji, Shan Gao<br>
					<em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br> 		
					<b><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Di_ShapeMatcher_Self-Supervised_Joint_Shape_Canonicalization_Segmentation_Retrieval_and_Deformation_CVPR_2024_paper.pdf">Paper</a></b> |
					<b><a href="https://github.com/Det1999/ShapeMaker">Code</a></b>
			</p></td></tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/kpred.PNG" height="140" width="240" alt="kpred"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation</strong><br>
					Ruida Zhang*, <span style="text-decoration: underline">Chenyangguang Zhang</span>*, Yan Di, Fabian Manhardt, Xingyu Liu, Federico Tombari, Xiangyang Ji<br>
					<em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br> 		
					<b><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_KP-RED_Exploiting_Semantic_Keypoints_for_Joint_3D_Shape_Retrieval_and_CVPR_2024_paper.pdf">Paper</a></b> |
					<b><a href="https://github.com/lolrudy/KP-RED">Code</a></b>
			</p></td></tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/ured.PNG" height="140" width="240" alt="ured"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds</strong><br>
						Yan Di*, <span style="text-decoration: underline">Chenyangguang Zhang</span>*, Ruida Zhang*, Fabian Manhardt, Yongzhi Su, Jason Rambach, Didier Stricker, Xiangyang Ji, Federico Tombari<br>
						<em>International Conference on Computer Vision (<b>ICCV</b>), 2023. <br> 
						<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Di_U-RED_Unsupervised_3D_Shape_Retrieval_and_Deformation_for_Partial_Point_ICCV_2023_paper.pdf"><b>Paper</b></a> |
						<b><a href="https://github.com/ZhangCYG/U-RED">Code</a></b>
			</p></td></tr>
	
	<tr>
		<td colspan="2">
			<b><font face="Arial" size="4" style="color: rgb(107, 13, 60)">
				Theme 3: MVS and Pose Estimation </font></b><br>
			</td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/sst.PNG" height="140" width="240" alt="sst"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong>SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse Spatial-Temporal Guidance</strong><br>
						<span style="text-decoration: underline">Chenyangguang Zhang*</span>, Zhiqiang Lou*, Yan Di, Federico Tombari, Xiangyang Ji<br>
						<em>IEEE International Conference on Multimedia and Expo (<b>ICME</b>)</em>, 2023 (<b>oral</b>). <br> 
					<a href="https://ieeexplore.ieee.org/abstract/document/10219928"><b>Paper</b></a>|
					<a href="figs\SST_ICME23_PPT.pdf"><b>Oral Presentation Slides</b></a>
			</p></td>
	</tr>
			
	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/gdrnpp.png" height="140" width="240" alt="gdrnpp"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong>GDRNPP: A Geometry-guided and Fully Learning-based Object Pose Estimator</strong><br>
			            Xingyu Liu, Ruida Zhang, <span style="text-decoration: underline">Chenyangguang Zhang</span>, Gu Wang, Jiwen Tang, Zhigang Li, Xiangyang Ji<br>
						<em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<b>TPAMI</b>)</em>, 2025. <br>
						<b>Winner of BOP Challenge 2022 @ ECCV R6D Workshop.</b><br>
					<a href="https://ieeexplore.ieee.org/abstract/document/10937121"><b>Paper</b></a>|
					<a href="https://cmp.felk.cvut.cz/sixd/workshop_2022/slides/bop_challenge_2022_results.pdf"><b>Slides</b></a> |
					<b><a href="https://github.com/shanice-l/gdrnpp_bop2022">Code</a></b>
			</p></td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/GPose2023.PNG" height="140" width="240" alt="gpose"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong>GPose2023: A Modularized Learning-based Object Pose Estimator</strong><br>
			            Ruida Zhang, Ziqin Huang, Gu Wang, Xingyu Liu, <span style="text-decoration: underline">Chenyangguang Zhang</span>, Xiangyang Ji<br>
						<em>International Conference on Computer Vision Workshop (<b>ICCVW</b>)</em>, 2023. <br>
						<b>Winner of BOP Challenge 2023 @ ICCV R6D Workshop.</b><br>
					<a href="figs\GPose_ICCV23.pdf"><b>Oral Presentation Slides</b></a>
			</p></td>
	</tr>

	</tbody>
</table>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
	Work Experiences and Interships</font></b></p>
	<font style="font-size: 12pt; line-height: 1.25;">
		<ul style="margin-left: 60px;margin-top: -10px">
			<li><b><a href="https://dexmal.com/">Dexmal</a></b>: Whole body control for quadrupeds, 2025</li>
			<li><b><a href="https://cvg.ethz.ch/index">CVG</a>, ETH Zurich</b>: Project mobility student for 3D computer vision, supervised by Prof. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>, and daily advised by Dr. <a href="https://francisengelmann.github.io/">Francis Engelmann</a> and <a href="https://fangjinhuawang.github.io/">Fangjinhua Wang</a>, 2024</li>
			<li><b>Definite Capital</b>: Machine learning for quantitative trades, where I worked with Dr. <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=1xn5GHgAAAAJ">Rui Xu</a> and Dr. <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=7P30Es0AAAAJ">Yu Xiong</a>, 2023</li>
			<li><b>TikTok</b>: 3D scene reconstruction for VR, where I worked with Dr. <a href="https://pengwangucla.github.io/peng-wang.github.io/">Peng Wang</a>, 2022</li>
			<li><b>LinkedSee</b>: Machine learning for AIOPS, where I worked with AP Dr. <a href="https://www.ai.pku.edu.cn/info/1138/2337.htm">Tong Jia</a>, 2022</li>
			<li><b>Meituan</b>: Real-time 3D mapping for UAV, where I'm supervised by Prof. Dr. <a href="https://udel.edu/~ghuang/">Guoquan Huang</a>, 2021</li>
			<li><b>Tencent Games</b>: Backend developer for game servers, 2021</li>
		</ul>
	</font>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
		Honors and Awards</font></b></p>
		<font style="font-size: 12pt; line-height: 1.25;">
			<ul style="margin-left: 60px;margin-top: -10px">
				<li>Li Yanda Scholarship, 2024</li>
				<li>Tianma Intelligent Control Scholarship, 2023</li>
				<li>China Optics Valley Scholarship, 2021</li>
				<li>Outstanding Student of THU, 2021</li>
				<li>Tang Lixin Excellent Scholarship, 2020.</li>
				<li>Good Reading Scholarship, THU, 2019.</li>
			</ul>
		</font>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
		Miscellaneous</font></b></p>
<p style="margin-left: 60px;margin-right: 60px;margin-top: -10px;font-size: 12pt">
	I love sports. I enjoy playing soccer&#9917 and badminton&#127992, etc. and serve as a winger of soccer team of DA, THU. I'm a soccer fans of Man. Utd. and José Mourinho.
	<br>
	<br>
	I'm also interested in writing novels and poems. Some of my works are published on Chinese literary magazines. My Wechat official account is named "了妄", where I share some literary works I wrote. 
	<br>
	<br>
	I'm a guitar player admiring Tommy Emmanuel and Kotaro Oshio. Meanwhile, I'm a figure skating fans of Yuzuru Hanyu.
	<br>
	<br>
	Feel free to get in touch!

</p>
<br>
<br>

</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-0.872831425-0.8"></script>
	<script>
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());

	gtag('config', 'UA-0.872831425-0.8');
	</script>

	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

	<meta content="en-us" http-equiv="Content-Language">
	<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
	<title>Chenyangguang Zhang</title>
	<meta charset="utf-8">
	<style type="text/css">
		.container {
			zoom: 1;
			margin-left: auto;
			margin-right: auto;
			vertical-align: middle;
			text-align: left;
			width: 100%;
			max-width: 800px;
		}
		body {font-family: Arial}
		a {text-decoration:none}
		a:any-link{color: darkred}
     	a:link{color:darkred;}
		a:visited{color:darkred;}
        a:hover{color:darkorange;}

	</style>
</head>


<body style="width:66%;margin:auto" bgcolor="#FFFFFF">
	<table border="0" id="table1" style="margin-left: 8px">
		<tbody>
			<tr>
				<td width="323">
					<p align="center"><font face="Arial"><img border="0" src="figs/me.jpg" height="224"></font></p>
				</td>
				<td >
					<p ><font face="Arial" style="font-size: 26pt;" >&nbsp;Chenyangguang (Cyrus) Zhang (张晨阳光)<span lang="zh-cn"></span></font></p>

					<p style="margin-top: 3mm;margin-bottom: 0.5mm"><font face="Arial" style="font-size: 12pt;" >&nbsp; Master student at Department of Automation, Tsinghua University</font></p>
					<p style="margin-top: 3mm;margin-bottom: 0.5mm"><font face="Arial" style="font-size: 12pt;" >&nbsp; Project mobility student at <a href="https://cvg.ethz.ch/index">CVG</a>, ETH Zurich</font></p>
					<p style="margin-top: 3mm;margin-bottom: 0.5mm"><font face="Arial" style="font-size: 12pt;">&nbsp; <b>Email</b>: zcyg22@mails.tsinghua.edu.cn</font></p>
					&nbsp;
					<a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=-sCslRcAAAAJ" style="font-size: 12pt;"><b>Google Scholar</b></a> &bull;
					<a href="https://github.com/ZhangCYG" style="font-size: 12pt;"><b>GitHub</b></a>&bull;
					<a href="https://www.linkedin.com/in/chenyangguangzhang/" style="font-size: 12pt;"><b>LinkedIn</b></a>
				</td>

			</tr>
		</tbody>
	</table>

<table style="margin-left: 55px;margin-right: 55px">
		<tbody>
			<td>
				<td style="border-style: none; border-width: medium;">
					<p style="margin-top: 3px; margin-bottom: 3px;"><font face="Arial" style="font-size: 12pt;">
						<br>I am a second-year Master student at BBNC Lab, Department of Automation, Tsinghua University, supervised by Professor <a href="https://www.au.tsinghua.edu.cn/info/1111/1524.htm"><b>Xiangyang Ji</b></a>.
						I got my bachelor's degree also from <a href="http://www.au.tsinghua.edu.cn/publish/auen/index.html"><b>Department of Automation</b></a>, Tsinghua University.
						<br>
						<br>
						
						My research interest lies in <b style="color: darkorange">3D computer vision</b>,
						especially <b style="color: darkorange">shape analysis and scene reconstruction</b>.
						I'm open for any related cooperation. Feel free to get in touch!
						<br>
						<br>

						Some of my close colllaborators include PD Dr. <a href="https://federicotombari.github.io/">Federico Tombari</a>, Dr. <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=bERItx8AAAAJ">Fabian Mandhart</a>, Dr. <a href="https://scholar.google.com.sg/citations?user=htu3c7wAAAAJ&hl=zh-CN">Gu Wang</a>, <a href="https://shangbuhuan13.github.io/">Yan Di</a>, <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=J4u6VicAAAAJ">Ruida Zhang</a>, <a href="https://lliu-xingyu.github.io/">Xingyu Liu</a>.
						<br>
						<br>
						
						I was invited as a reviewer for ECCV 2024, ICCV 2023, AISTAT 2024. I'm a student member of Chinese Society of Image and Graphics (CSIG).
						<br>
						<br>

						<b> Currently, I'm willing to apply for a PhD position in Fall, 2025.</b>
						<br>
						<br>
					</font></p>
				</td>
			</td>
		</tbody>
</table>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
		News</font></b></p>
		<font style="font-size: 12pt; line-height: 1.25;">
			<ul style="margin-left: 60px;margin-top: -10px">
				<li>[Feb, 2024] Three my (co-)first-authored papers are accepted by <b>CVPR 2024</b>, including MOHO for single-view hand-held object reconstruction without 3D supervision; KP-RED for shape analysis without part annotations; ShapeMaker for indoor shape analysis from arbitrary poses.</li>
				<li>[Feb, 2024] I'm approved to enroll as an project mobility student to <b>CVG, ETH Zurich</b>. Great thanks for Prof. Marc Pollefeys for giving this opportunity!</li>
				<li>[Oct, 2023] Our work GPose2023 <b>wins BOP Challenge 2023</b>, ICCV R6D Workshop. It's honored for me to give an oral presentation in <b>ICCV 2023</b>.</li>
				<li>[Sept, 2023] My first-authored paper DDF-HO on single-view hand-held object reconstruction is accepted by <b>NeurIPS 2023</b>.</li>
				<li>[July, 2023] My co-first-authored paper U-RED on unsupervised shape retrieval and deformation in indoor scenes is accepted by <b>ICCV 2023</b>.</li>
				<li>[July, 2023] I have an oral presentation about paper SST in <b>ICME 2023</b>.</li>
				<li>[March, 2023] My first-authored paper SST on end-to-end real-time indoor scene reconstruction is accepted by <b>ICME 2023 (oral)</b>.</li>
				<li>[October, 2022] Our GDRNPP on object pose estimation wins the majority of <b>best awards in BOP Challenge 2022, ECCV R6D Workshop</b>.</li>
			</ul>
		</font>

<table style="width:80%;margin-left: 55px;border-spacing: 0;">
	<tbody>
	<tr>
		<p style="margin-left: 60px;margin-bottom: 10px"><b><font face="Arial" size="5">
		Selected Publications and Contests</font></b></p>
	</tr>

	<tr>
		<td colspan="2">
			<b><font face="Arial" size="4" style="color: rgb(107, 13, 60)">
				Theme 1: Shape Analysis with Human Interaction </font></b><br>
			</td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/MOHO.PNG" height="140" width="240" alt="MOHO"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision</strong><br>
					<span style="text-decoration: underline">Chenyangguang Zhang</span>, Guanlong Jiao, Yan Di, Gu Wang, Ziqin Huang, Ruida Zhang, Fabian Manhardt, Bowen Fu, Federico Tombari, Xiangyang Ji<br>
					<em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br> 		
					<b>Paper</b> |
					<b>Code</b>
			</p></td></tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/ddfho.PNG" height="140" width="240" alt="ddfho"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field</strong><br>
					<span style="text-decoration: underline">Chenyangguang Zhang</span>, Yan Di, Ruida Zhang, Guangyao Zhai, Fabian Manhardt, Federico Tombari, Xiangyang Ji<br>
					<em>Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2023. <br> 		
					<a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/b2876deb92cbd098219a10da25671577-Paper-Conference.pdf"><b>Paper</b></a> |
					<b><a href="https://github.com/ZhangCYG/DDFHO">Code</a></b> |
					<a href="figs\ddfho_poster.pdf"><b>Poster</b></a>
			</p></td></tr>

	<tr>
		<td colspan="2">
			<b><font face="Arial" size="4" style="color: rgb(107, 13, 60)">
				Theme 2: Indoor Shape Analysis</font></b><br>
			</td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/shapemaker.PNG" height="140" width="240" alt="shapemaker"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >ShapeMaker: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation</strong><br>
					Yan Di*, <span style="text-decoration: underline">Chenyangguang Zhang</span>*, Chaowei Wang*, Ruida Zhang, Guangyao Zhai, Yanyan Li, Bowen Fu, Xiangyang Ji, Shan Gao<br>
					<em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br> 		
					<b>Paper</b> |
					<b>Code</b>
			</p></td></tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/kpred.PNG" height="140" width="240" alt="kpred"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation</strong><br>
					Ruida Zhang*, <span style="text-decoration: underline">Chenyangguang Zhang</span>*, Yan Di*, Fabian Manhardt, Xingyu Liu, Federico Tombari, Xiangyang Ji<br>
					<em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br> 		
					<b>Paper</b> |
					<b>Code</b>
			</p></td></tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/ured.PNG" height="140" width="240" alt="ured"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong >U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds</strong><br>
						Yan Di*, <span style="text-decoration: underline">Chenyangguang Zhang</span>*, Ruida Zhang*, Fabian Manhardt, Yongzhi Su, Jason Rambach, Didier Stricker, Xiangyang Ji, Federico Tombari<br>
						<em>International Conference on Computer Vision (<b>ICCV</b>), 2023. <br> 
						<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Di_U-RED_Unsupervised_3D_Shape_Retrieval_and_Deformation_for_Partial_Point_ICCV_2023_paper.pdf"><b>Paper</b></a> |
						<b><a href="https://github.com/ZhangCYG/U-RED">Code</a></b>
			</p></td></tr>
	
	<tr>
		<td colspan="2">
			<b><font face="Arial" size="4" style="color: rgb(107, 13, 60)">
				Theme 3: Scene Reconstruction and Pose Estimation </font></b><br>
			</td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/sst.PNG" height="140" width="240" alt="sst"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong>SST: Real-time End-to-end Monocular 3D Reconstruction via Sparse Spatial-Temporal Guidance</strong><br>
						<span style="text-decoration: underline">Chenyangguang Zhang</span>, Zhiqiang Lou, Yan Di, Federico Tombari, Xiangyang Ji<br>
						<em>IEEE International Conference on Multimedia and Expo (<b>ICME</b>)</em>, 2023 (<b>oral</b>). <br> 
					<a href="https://ieeexplore.ieee.org/abstract/document/10219928"><b>Paper</b></a>|
					<a href="figs\SST_ICME23_PPT.pdf"><b>Oral Presentation Slides</b></a>
			</p></td>
	</tr>
			
	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/gdrnpp.png" height="140" width="240" alt="gdrnpp"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong>GDRNPP: Extending Geometry-Guided Direct Regression Network in 2022</strong><br>
			            Xingyu Liu, Ruida Zhang, <span style="text-decoration: underline">Chenyangguang Zhang</span>, Bowen Fu, Jiwen Tang, Xiquan Liang, Jingyi Tang, Xiaotian Cheng, Yukang Zhang, Gu Wang, Xiangyang Ji<br>
						<em>European Conference on Computer Vision Workshop (<b>ECCVW</b>)</em>, 2022. <br>
						<b>Winner of BOP Challenge 2022 @ ECCV R6D Workshop.</b><br>
					<a href="https://cmp.felk.cvut.cz/sixd/workshop_2022/slides/bop_challenge_2022_results.pdf"><b>Slides</b></a> |
					<b><a href="https://github.com/shanice-l/gdrnpp_bop2022">Code</a></b>
			</p></td>
	</tr>

	<tr align="left">
		<td width="20%" style="vertical-align: middle;"><p><img src="./figs/GPose2023.PNG" height="140" width="240" alt="gpose"></p></td>
		<td width="80%" style="vertical-align: middle;"><p style="font-size: 12pt;line-height: 1.2;width: 16cm; margin-left: 10px;">
			<strong>GPose2023: A Modularized Learning-based Object Pose Estimator</strong><br>
			            Ruida Zhang, Ziqin Huang, Gu Wang, Xingyu Liu, <span style="text-decoration: underline">Chenyangguang Zhang</span>, Xiangyang Ji<br>
						<em>International Conference on Computer Vision Workshop (<b>ICCVW</b>)</em>, 2023. <br>
						<b>Winner of BOP Challenge 2023 @ ICCV R6D Workshop.</b><br>
					<a href="figs\GPose_ICCV23.pdf"><b>Oral Presentation Slides</b></a>
			</p></td>
	</tr>

	</tbody>
</table>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
	Work Experiences and Interships</font></b></p>
	<font style="font-size: 12pt; line-height: 1.25;">
		<ul style="margin-left: 60px;margin-top: -10px">
			<li><b><a href="https://cvg.ethz.ch/index">CVG</a>, ETH Zurich</b>: Project mobility student for 3D scene analysis, supervised by Prof. <a href="https://people.inf.ethz.ch/pomarc/">Marc Pollefeys</a>, and daily advised by <a href="https://fangjinhuawang.github.io/">Fangjinhua Wang</a> and Dr. <a href="https://francisengelmann.github.io/">Francis Engelmann</a>, 2024</li>
			<li><b>Definite Capital</b>: Machine learning for quantitative trades, where I worked with Dr. <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=1xn5GHgAAAAJ">Rui Xu</a> and Dr. <a href="https://scholar.google.com.sg/citations?hl=zh-CN&user=7P30Es0AAAAJ">Yu Xiong</a>, 2023</li>
			<li><b>TikTok</b>: 3D scene reconstruction for VR, where I worked with Dr. <a href="https://pengwangucla.github.io/peng-wang.github.io/">Peng Wang</a>, 2022</li>
			<li><b>LinkedSee</b>: Machine learning for AIOPS, where I worked with AP Dr. <a href="https://www.ai.pku.edu.cn/info/1138/2337.htm">Tong Jia</a>, 2022</li>
			<li><b>Meituan</b>: Real-time 3D mapping for UAV, where I'm supervised by Prof. <a href="https://udel.edu/~ghuang/">Guoquan Huang</a>, 2021</li>
			<li><b>Tencent Games</b>: Backend developer for game servers, where I worked with Gang Wang, 2021</li>
		</ul>
	</font>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
		Honors and Awards</font></b></p>
		<font style="font-size: 12pt; line-height: 1.25;">
			<ul style="margin-left: 60px;margin-top: -10px">
				<li>Tianma Intelligent Control Scholarship, 2023</li>
				<li>China Optics Valley Scholarship, 2021</li>
				<li>Outstanding Student of THU, 2021</li>
				<li>Tang Lixin Excellent Scholarship, 2020.</li>
				<li>Excellent Student Cadres of THU Student Union, 2020</li>
				<li>Good Reading Scholarship, THU, 2019.</li>
			</ul>
		</font>

<p style="margin-left: 60px;margin-top: -30px"><b><font size="5"><br>
		Miscellaneous</font></b></p>
<p style="margin-left: 60px;margin-right: 60px;margin-top: -10px;font-size: 12pt">
	I love sports. I enjoy playing soccer&#9917 and badminton&#127992, etc. and serve as a winger of soccer team of DA, THU. I'm a soccer fans of Man. Utd. and José Mourinho.
	<br>
	<br>
	I'm also interested in writing novels and poems. Some of my works are published on Chinese literary magazines. My Wechat official account is named "了妄", where I share some literary works I wrote. 
	<br>
	<br>
	I'm a guitar player admiring Tommy Emmanuel and Kotaro Oshio. Meanwhile, I'm a figure skating fans of Yuzuru Hanyu.
	<br>
	<br>
	Feel free to get in touch!

</p>
<br>
<br>

</body>
</html>